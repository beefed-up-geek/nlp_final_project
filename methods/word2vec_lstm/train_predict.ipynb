{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Prediction: Word2Vec + Bi-LSTM (GPU Optimized)\n",
    "\n",
    "This notebook implements MBTI personality prediction using:\n",
    "- **Embedding**: Word2Vec (trained on corpus)\n",
    "- **Model**: Bidirectional LSTM\n",
    "- **Task**: 4 binary classifications (E/I, N/S, T/F, P/J)\n",
    "- **Optimization**: NVIDIA RTX 3090 (24GB VRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import mixed_precision\n\n# Word2Vec\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK data\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\n\n# GPU Configuration for RTX 3090\nprint(\"Setting up GPU...\")\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Enable memory growth\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        \n        # Enable mixed precision for faster training\n        mixed_precision.set_global_policy('mixed_float16')\n        \n        print(f\"GPU Available: {len(gpus)} GPU(s)\")\n        print(f\"GPU Name: {gpus[0].name}\")\n        print(f\"Mixed Precision: Enabled (float16)\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU found, using CPU\")\n\nprint(f\"\\nTensorFlow version: {tf.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from GitHub\n",
    "TRAIN_URL = 'https://raw.githubusercontent.com/beefed-up-geek/nlp_final_project/refs/heads/main/kaggle_data/2025MBTItrain.csv'\n",
    "TEST_URL = 'https://raw.githubusercontent.com/beefed-up-geek/nlp_final_project/refs/heads/main/kaggle_data/2025test.csv'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_URL)\n",
    "test_df = pd.read_csv(TEST_URL)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nMBTI type distribution:\")\n",
    "print(train_df['type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing texts...\")\n",
    "train_df['cleaned_posts'] = train_df['posts'].apply(preprocess_text)\n",
    "test_df['cleaned_posts'] = test_df['posts'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['E_I'] = train_df['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "train_df['N_S'] = train_df['type'].apply(lambda x: 1 if x[1] == 'N' else 0)\n",
    "train_df['T_F'] = train_df['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n",
    "train_df['P_J'] = train_df['type'].apply(lambda x: 1 if x[3] == 'P' else 0)\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "for col in ['E_I', 'N_S', 'T_F', 'P_J']:\n",
    "    dist = train_df[col].value_counts()\n",
    "    print(f\"{col}: 0={dist[0]}, 1={dist[1]} (ratio: {dist[1]/len(train_df):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters optimized for 3090 GPU\n",
    "MAX_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['cleaned_posts'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['cleaned_posts'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['cleaned_posts'])\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Train sequences shape: {X_train_padded.shape}\")\n",
    "print(f\"Test sequences shape: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model...\")\n",
    "all_texts = pd.concat([train_df['cleaned_posts'], test_df['cleaned_posts']])\n",
    "tokenized_texts = [word_tokenize(text) for text in all_texts]\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=8,  # Use multiple CPU cores\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec trained: {len(w2v_model.wv)} words\")\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, MAX_WORDS)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx >= MAX_WORDS:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(0, 0.05, EMBEDDING_DIM)\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Coverage: {np.count_nonzero(embedding_matrix.sum(axis=1))} / {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm_model(embedding_matrix, max_length):\n",
    "    \"\"\"GPU-optimized Bi-LSTM model\"\"\"\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    \n",
    "    embedding = Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=True\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Bi-LSTM layers\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(embedding)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output (float32 for mixed precision)\n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for 3090 GPU\n",
    "BATCH_SIZE = 64  # Optimized for 24GB VRAM\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "dimensions = ['E_I', 'N_S', 'T_F', 'P_J']\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {dim} classifier\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    y = train_df[dim].values\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train_padded, y, \n",
    "        test_size=VALIDATION_SPLIT, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = create_bilstm_model(embedding_matrix, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_tr, y_tr,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"\\n✓ Best Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    models[dim] = model\n",
    "    histories[dim] = history\n",
    "    \n",
    "    # Clear memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions...\")\n",
    "predictions = {}\n",
    "\n",
    "for dim in dimensions:\n",
    "    pred_proba = models[dim].predict(X_test_padded, batch_size=BATCH_SIZE, verbose=0)\n",
    "    pred_binary = (pred_proba > 0.5).astype(int).flatten()\n",
    "    predictions[dim] = pred_binary\n",
    "    print(f\"{dim}: {np.bincount(pred_binary)}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'E_I': predictions['E_I'],\n",
    "    'N_S': predictions['N_S'],\n",
    "    'T_F': predictions['T_F'],\n",
    "    'P_J': predictions['P_J']\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'submission_word2vec_lstm.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"✓ Submission saved: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for dim in dimensions:\n",
    "    best_acc = max(histories[dim].history['val_accuracy'])\n",
    "    print(f\"{dim}: {best_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}