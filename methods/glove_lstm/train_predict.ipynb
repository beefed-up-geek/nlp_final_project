{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Prediction: GloVe + Bi-LSTM (GPU Optimized)\n",
    "\n",
    "This notebook implements MBTI personality prediction using:\n",
    "- **Embedding**: GloVe (pretrained)\n",
    "- **Model**: Bidirectional LSTM\n",
    "- **Task**: 4 binary classifications (E/I, N/S, T/F, P/J)\n",
    "- **Optimization**: NVIDIA RTX 3090 (24GB VRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport re\nimport os\nimport urllib.request\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import mixed_precision\n\nimport nltk\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\n\n# GPU Configuration\nprint(\"Setting up GPU...\")\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        mixed_precision.set_global_policy('mixed_float16')\n        print(f\"GPU: {len(gpus)} device(s) | Mixed Precision: Enabled\")\n    except RuntimeError as e:\n        print(e)\n\nprint(f\"TensorFlow: {tf.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = 'https://raw.githubusercontent.com/beefed-up-geek/nlp_final_project/refs/heads/main/kaggle_data/2025MBTItrain.csv'\n",
    "TEST_URL = 'https://raw.githubusercontent.com/beefed-up-geek/nlp_final_project/refs/heads/main/kaggle_data/2025test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_URL)\n",
    "test_df = pd.read_csv(TEST_URL)\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "train_df['cleaned_posts'] = train_df['posts'].apply(preprocess_text)\n",
    "test_df['cleaned_posts'] = test_df['posts'].apply(preprocess_text)\n",
    "\n",
    "train_df['E_I'] = train_df['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "train_df['N_S'] = train_df['type'].apply(lambda x: 1 if x[1] == 'N' else 0)\n",
    "train_df['T_F'] = train_df['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n",
    "train_df['P_J'] = train_df['type'].apply(lambda x: 1 if x[3] == 'P' else 0)\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['cleaned_posts'])\n",
    "\n",
    "X_train_padded = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_df['cleaned_posts']),\n",
    "    maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post'\n",
    ")\n",
    "X_test_padded = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_df['cleaned_posts']),\n",
    "    maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_padded.shape}, Test: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download and Load GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe embeddings if not exists\n",
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_DIR = 'glove_embeddings'\n",
    "EMBEDDING_DIM = 100\n",
    "GLOVE_FILE = f'{GLOVE_DIR}/glove.6B.{EMBEDDING_DIM}d.txt'\n",
    "\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    os.makedirs(GLOVE_DIR, exist_ok=True)\n",
    "    zip_path = f'{GLOVE_DIR}/glove.6B.zip'\n",
    "    \n",
    "    urllib.request.urlretrieve(GLOVE_URL, zip_path)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(GLOVE_DIR)\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"GloVe embeddings already exist\")\n",
    "\n",
    "# Load GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Loaded {len(embeddings_index)} word vectors\")\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, MAX_WORDS)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "hits = 0\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(0, 0.05, EMBEDDING_DIM)\n",
    "\n",
    "print(f\"Embedding matrix: {embedding_matrix.shape}\")\n",
    "print(f\"Coverage: {hits}/{vocab_size} ({hits/vocab_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm_model(embedding_matrix, max_length):\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    \n",
    "    embedding = Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=True\n",
    "    )(input_layer)\n",
    "    \n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(embedding)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "dimensions = ['E_I', 'N_S', 'T_F', 'P_J']\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {dim}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    y = train_df[dim].values\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train_padded, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    model = create_bilstm_model(embedding_matrix, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_tr, y_tr,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"✓ Best Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    models[dim] = model\n",
    "    histories[dim] = history\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\nAll models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for dim in dimensions:\n",
    "    pred_proba = models[dim].predict(X_test_padded, batch_size=BATCH_SIZE, verbose=0)\n",
    "    predictions[dim] = (pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'E_I': predictions['E_I'],\n",
    "    'N_S': predictions['N_S'],\n",
    "    'T_F': predictions['T_F'],\n",
    "    'P_J': predictions['P_J']\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_glove_lstm.csv', index=False)\n",
    "print(\"✓ Submission saved: submission_glove_lstm.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for dim in dimensions:\n",
    "    print(f\"{dim}: {max(histories[dim].history['val_accuracy']):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}